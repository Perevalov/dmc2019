{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from utils.ScoreFunction import score_function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "from utils.paramsearch import paramsearch\n",
    "from itertools import product,chain\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/train.csv\",sep='|')\n",
    "X = data.drop([\"fraud\"],axis=1)\n",
    "y = data[\"fraud\"]\n",
    "\n",
    "X['totalScanned'] = X['scannedLineItemsPerSecond']*X['totalScanTimeInSeconds']\n",
    "X['avgTimePerScan'] = 1/X['scannedLineItemsPerSecond']\n",
    "X['avgValuePerScan'] = X['avgTimePerScan']*X['valuePerSecond']\n",
    "X['withoutRegisPerPosition'] = X['scansWithoutRegistration']*X['totalScanned']\n",
    "X['quantityModsPerPosition'] = X['quantityModifications']/X['totalScanned']\n",
    "\n",
    "#X['lineItemVoidsPerPosition'] = X['lineItemVoids']/X['totalScanned'] #\n",
    "#X['lineItemVoidsPerTotal'] = X['lineItemVoids']/X['grandTotal'] #\n",
    "#X['withoutRegistrationPerTotal'] = X['scansWithoutRegistration']/X['grandTotal'] #\n",
    "#X['quantiModsPerTotal'] = X['quantityModifications']/X['grandTotal'] #\n",
    "#X['lineItemVoidsPerTime'] = X['lineItemVoids']/X['totalScanTimeInSeconds'] #\n",
    "#X['withoutRegistrationPerTime'] = X['scansWithoutRegistration']/X['totalScanTimeInSeconds'] #\n",
    "#X['quantiModesPerTime'] = X['quantityModifications']/X['totalScanTimeInSeconds'] #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest_xg(params, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = np.array(clf.predict(X_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "def xgboost_param_tune(params, X, y ,n_splits=5):\n",
    "    ps = paramsearch(params_xg)\n",
    "    for prms in chain(ps.grid_search(['n_estimators','learning_rate']),\n",
    "                      ps.grid_search(['max_depth','min_child_weight'])):\n",
    "        res = crossvaltest_xg(prms,X, y,n_splits)\n",
    "        ps.register_result(res,prms)\n",
    "        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "        print()\n",
    "    return ps.bestparam(), ps.bestscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest_cat(params, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = CatBoostClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = np.array(clf.predict(X_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "def cat_param_tune(params, X, y ,n_splits=5):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(ps.grid_search(['border_count']),\n",
    "                      ps.grid_search(['l2_leaf_reg']),\n",
    "                      ps.grid_search(['iterations','learning_rate']),\n",
    "                      ps.grid_search(['depth'])):\n",
    "        res = crossvaltest_cat(prms,X, y,n_splits)\n",
    "        ps.register_result(res,prms)\n",
    "        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "        print()\n",
    "    return ps.bestparam(), ps.bestscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA_BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest_ada(params, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = AdaBoostClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = np.array(clf.predict(X_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "def ada_param_tune(params, X, y ,n_splits=5):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(ps.grid_search(['n_estimators', 'learning_rate', 'algorithm'])):\n",
    "        res = crossvaltest_ada(prms,X, y,n_splits)\n",
    "        ps.register_result(res,prms)\n",
    "        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "        print()\n",
    "    return ps.bestparam(), ps.bestscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest_bag(params, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = BaggingClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = np.array(clf.predict(X_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "def bag_param_tune(params, X, y ,n_splits=5):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(ps.grid_search(['n_estimators', 'max_samples'])):\n",
    "        res = crossvaltest_bag(prms,X, y,n_splits)\n",
    "        ps.register_result(res,prms)\n",
    "        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "        print()\n",
    "    return ps.bestparam(), ps.bestscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC_REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest_log_reg(params, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = LogisticRegression(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = np.array(clf.predict(X_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        \n",
    "    return np.mean(score)\n",
    "\n",
    "def log_reg_param_tune(params, X, y ,n_splits=5):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(ps.grid_search(['C'])):\n",
    "        res = crossvaltest_log_reg(prms,X, y,n_splits)\n",
    "        ps.register_result(res,prms)\n",
    "        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "        print()\n",
    "    return ps.bestparam(), ps.bestscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(params):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy, score, f1 = [], [], []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train_dnn = sc.fit_transform(X_train)\n",
    "        X_test_dnn = sc.transform(X_test)\n",
    "        clf = Sequential()\n",
    "        clf.add(Dense(128, activation='relu', kernel_initializer='random_normal', input_dim=14))\n",
    "        clf.add(Dense(128, activation='relu', kernel_initializer='random_normal'))\n",
    "        clf.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "        clf.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "        clf.fit(X_train_dnn, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose = False)\n",
    "\n",
    "        y_pred = np.array(clf.predict(X_test_dnn))\n",
    "        y_pred[y_pred>0.5] = 1\n",
    "        y_pred[y_pred<0.5] = 0\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        score.append(score_function(tp,fp,fn,tn))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL_TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-64.0 {'C': 0.01} best: -64.00000000000446 {'C': 0.01}\n",
      "\n",
      "36.0 {'C': 0.1} best: 36.000000000136325 {'C': 0.1}\n",
      "\n",
      "46.0 {'C': 1} best: 46.000000000050775 {'C': 1}\n",
      "\n",
      "44.0 {'C': 10} best: 46.000000000050775 {'C': 1}\n",
      "\n",
      "45.0 {'C': 100} best: 46.000000000050775 {'C': 1}\n",
      "\n",
      "43.0 {'C': 1000} best: 46.000000000050775 {'C': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.001, 'min_child_weight': 1} best: -104.00000000007043 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.001, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.01, 'min_child_weight': 1} best: -104.00000000001448 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.01, 'min_child_weight': 1}\n",
      "\n",
      "18.0 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.1, 'min_child_weight': 1} best: 17.999999999928836 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-16.0 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.03, 'min_child_weight': 1} best: 17.999999999928836 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.001, 'min_child_weight': 1} best: 17.999999999928836 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.01, 'min_child_weight': 1} best: 17.999999999928836 {'max_depth': 1, 'n_estimators': 100, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "41.0 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.1, 'min_child_weight': 1} best: 40.99999999986036 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "16.0 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.03, 'min_child_weight': 1} best: 40.99999999986036 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.001, 'min_child_weight': 1} best: 40.99999999986036 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.01, 'min_child_weight': 1} best: 40.99999999986036 {'max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "42.0 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.1, 'min_child_weight': 1} best: 42.00000000007948 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "17.0 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.03, 'min_child_weight': 1} best: 42.00000000007948 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.001, 'min_child_weight': 1} best: 42.00000000007948 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.01, 'min_child_weight': 1} best: 42.00000000007948 {'max_depth': 1, 'n_estimators': 300, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "44.0 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.1, 'min_child_weight': 1} best: 44.000000000181856 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "24.0 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.03, 'min_child_weight': 1} best: 44.000000000181856 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.001, 'min_child_weight': 1} best: 44.000000000181856 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-62.0 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.01, 'min_child_weight': 1} best: 44.000000000181856 {'max_depth': 1, 'n_estimators': 400, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "48.0 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.1, 'min_child_weight': 1} best: 48.00000000004661 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "34.0 {'max_depth': 1, 'n_estimators': 600, 'learning_rate': 0.03, 'min_child_weight': 1} best: 48.00000000004661 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n",
      "-104.0 {'max_depth': 1, 'n_estimators': 600, 'learning_rate': 0.001, 'min_child_weight': 1} best: 48.00000000004661 {'max_depth': 1, 'n_estimators': 500, 'learning_rate': 0.1, 'min_child_weight': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params_xg = {'max_depth':[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "             'n_estimators':[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100],\n",
    "             'learning_rate':[0.03,0.001,0.01,0.1], \n",
    "             'min_child_weight': [1,2,3,4]}\n",
    "\n",
    "params_cat = {'depth':[1,3,5,7],\n",
    "              'iterations':[100, 200,400,600,800,1000,2000],\n",
    "              'learning_rate':[0.03,0.001,0.01,0.1], \n",
    "              'l2_leaf_reg':[1,5,10,100],\n",
    "              'border_count':[2,5,10,20,50,100],\n",
    "              'thread_count':4,\n",
    "              'silent': True}\n",
    "\n",
    "params_ada = {'learning_rate':[0.03,0.001,0.01,0.1],\n",
    "              'n_estimators':[100, 300, 500, 700, 900, 1000],\n",
    "              'algorithm': ['SAMME', 'SAMME.R']}\n",
    "\n",
    "params_log_reg = {'C':[0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "params_dnn = {'epochs': 500, 'batch_size': 32}\n",
    "\n",
    "params_log_reg, best_log_reg = log_reg_param_tune(params_log_reg,X, y)\n",
    "params_xg_boost, best_xg = xgboost_param_tune(params_xg,X, y)\n",
    "params_ada, best_ada = ada_param_tune(params_ada,X, y)\n",
    "params_cat, best_cat = cat_param_tune(params_cat,X, y)\n",
    "best_dnn = DNN(params_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL_TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score = 59.0\n",
      "Mean Accuracy = 0.990418439716312\n",
      "Mean F1_score = 0.906794425087108\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "accuracy, score, f1 = [], [], []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    clf = LogisticRegression(**params_log_reg)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_log_reg = np.array(clf.predict(X_test))\n",
    "    \n",
    "    clf = XGBClassifier(**params_xg_boost)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_xg = np.array(clf.predict(X_test))\n",
    "    \n",
    "    clf = AdaBoostClassifier(**params_ada)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_ada = np.array(clf.predict(X_test))\n",
    "    \n",
    "    clf = CatBoostClassifier(**params_cat)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_cat = np.array(clf.predict(X_test))\n",
    "    \n",
    "    X_train_dnn = sc.fit_transform(X_train)\n",
    "    X_test_dnn = sc.transform(X_test)\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(128, activation='relu', kernel_initializer='random_normal', input_dim=14))\n",
    "    clf.add(Dense(128, activation='relu', kernel_initializer='random_normal'))\n",
    "    clf.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "    clf.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    clf.fit(X_train_dnn, y_train, epochs=params_dnn['epochs'], batch_size=params_dnn['batch_size'], verbose = False)\n",
    "    y_pred_dnn = np.array(clf.predict(X_test_dnn))\n",
    "    y_pred_dnn[y_pred_dnn>0.5] = 1\n",
    "    y_pred_dnn[y_pred_dnn<0.5] = 0\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(len(y_pred_cat)):\n",
    "        temp_prediction = [float(y_pred_log_reg[i]), float(y_pred_xg[i]), float(y_pred_ada[i]), float(y_pred_cat[i]), float(y_pred_dnn[i])]\n",
    "        y_pred.append(most_common(temp_prediction))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    score.append(score_function(tp,fp,fn,tn))\n",
    "    f1.append(f1_score(y_test, y_pred))\n",
    "\n",
    "print('Mean Score = {0}'.format(np.mean(score)))\n",
    "print('Mean Accuracy = {0}'.format(np.mean(accuracy)))\n",
    "print('Mean F1_score = {0}'.format(np.mean(f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
